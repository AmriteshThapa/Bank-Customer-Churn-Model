# -*- coding: utf-8 -*-
"""amriteshthapaaassignment

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l-DEWQX86-moCL1t54fMsJ53yj4i8LNw

# **Bank Customer Churn Model**

---

1.   data encoding
2.   feature scaling
3.   handling imbalance
  *   random under sampling
  *   random over sampling
4.   support vector machine classifier
5.   grid search for hyperparameter tunning

# Import Library
"""

import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

"""# Import Data"""

df = pd.read_csv('https://github.com/YBI-Foundation/Dataset/raw/main/Bank%20Churn%20Modelling.csv')

"""# Analyse Data"""

df.head()

df.info()

df.duplicated('CustomerId').sum()

df = df.set_index('CustomerId')

df.info()

"""# Encoding"""

df['Geography'].value_counts()

df.replace({'Geography':{'France': 2,'Germany':1,'Spain':0}}, inplace=True)

df['Gender'].value_counts()

df.replace({'Gender':{'Male':0,'Female':1}},inplace=True)

df['Num Of Products'].value_counts()

df.replace({'Num Of Products': {1:0,2:1,3:1,4:1}},inplace=True)

df['Has Credit Card'].value_counts()

df['Is Active Member'].value_counts()

df.loc[(df['Balance']==0), 'Churn'].value_counts()

df['Zero Balance'] = np.where(df['Balance']>0,1,0)

df['Zero Balance'].hist()

df.groupby(['Churn','Geography']).count()

"""# Define Label and Features"""

df.columns

x=df.drop(['Surname','Churn'],axis=1)

y=df['Churn']

x.shape,y.shape

"""# Handling Imbalance Data
Class imbalance is a common problem in machine learning especially in classification problems as machine learning algorithms are design to maximize accuracy and reduce errors. If the data set is imbalance then in such cases just by predicting the majority classes we get a pretty high accuracy but fails to capture the minority class which is most often the point of creating model in the first place. Like in
1. **Fraud detaction**
2. **Spam filtering**
3. **Diseases screening**
4. **Online sales churn**
5. **Advertising click-throughs**


**Under sampling** can be defined as removing some observations of the majority class. This is done until the majority and minority class is balanced out. Understanding can be good choice when you have turn off think million of rose. But it drawback to understanding is that we are removing information that may be valuable. In under sampling the simplest technique involves removing random records from majority class which can cause loss of information. A drawback to consider when under sampling is that it can cause over fitting and poor generalisation to your test set.

**Oersampling** can be defined as adding more copies to minority class. Oversampling can be good choice when you don't have a ton of data to work with full stop the simply implementation of our sampling is to duplicate random records from the minority classes which can cause overitting
"""

df['Churn'].value_counts()

sns.countplot(x='Churn',data = df);

x.shape, y.shape

"""# Random Under Sampling"""

from imblearn.under_sampling import RandomUnderSampler

rus=RandomUnderSampler(random_state=2529)

x_rus, y_rus = rus.fit_resample(x,y)

x_rus.shape, y_rus.shape,x.shape,y.shape

y.value_counts()

y_rus.value_counts()

y_rus.plot(kind='hist')

"""# Random Over Sampling"""

from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(random_state=2529)

x_ros, y_ros= ros.fit_resample(x,y)

x_ros.shape, y_ros.shape,x.shape,y.shape

y.value_counts()

y_ros.plot(kind='hist')

"""# Train Test Split"""

from sklearn.model_selection import train_test_split

"""# Split Original Data"""

x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=2529)

"""# Split Random Under Sample Data"""

x_train_rus, x_test_rus, y_train_rus, y_test_rus = train_test_split(x_rus,y_rus, test_size=0.3, random_state=2529)

"""# Split Random Over Sampling Data"""

x_train_ros, x_test_ros, y_train_ros, y_test_ros = train_test_split(x_ros,y_ros, test_size=0.3, random_state=2529)

"""# Standardise Features"""

from sklearn.preprocessing import StandardScaler

sc =StandardScaler()

"""# Standardize Original Data"""

x_train[['CreditScore','Age','Tenure','Balance','Estimated Salary']] = sc.fit_transform(x_train[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

x_test[['CreditScore','Age','Tenure','Balance','Estimated Salary']] = sc.fit_transform(x_test[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

"""# Standardize Random Under Sample Data"""

x_train_ros[['CreditScore','Age','Tenure','Balance','Estimated Salary']] = sc.fit_transform(x_train_ros[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

x_test_ros[['CreditScore','Age','Tenure','Balance','Estimated Salary']] = sc.fit_transform(x_test_ros[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

"""# Standardize Random Over Sample Data"""

x_train_ros[['CreditScore','Age','Tenure','Balance','Estimated Salary']] = sc.fit_transform(x_train_ros[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

x_test_rus[['CreditScore','Age','Tenure','Balance','Estimated Salary']] = sc.fit_transform(x_test_rus[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

"""# Support Vector Machine Classifier"""

from sklearn.svm import SVC

svc=SVC()

svc.fit(x_train,y_train)

y_pred=svc.predict(x_test)

"""# Model Accuracy"""

from sklearn.metrics import confusion_matrix, classification_report

confusion_matrix(y_test,y_pred)

print(classification_report(y_test,y_pred))

"""# Hyperparameter Tuning"""

from sklearn.model_selection import GridSearchCV

param_grid={'C':[0.1,1,10],
            'gamma':[1,0.1,0.01],
            'kernel':['rbf'],
            'class_weight':['balanced']}

grid=GridSearchCV(SVC(),param_grid,refit=True,verbose=2,cv=2)
grid.fit(x_train,y_train)

print(grid.best_estimator_)

grid_predicton=grid.predict(x_test)

confusion_matrix(y_test,grid_predicton)

print(classification_report(y_test,grid_predicton))

"""# Model with Random Under Sampling"""

svc_rus=SVC()

svc_rus.fit(x_train_rus,y_train_rus)

y_pred_rus=svc_rus.predict(x_test_rus)

"""# Model Accuracy"""

confusion_matrix(y_test_rus,y_pred_rus)

print(classification_report(y_test_rus,y_pred_rus))

"""# Hyperparameter Tunning"""

param_grid={'C':[0.1,1,10],
            'gamma':[1,0.1,0.01],
            'kernel':['rbf'],
            'class_weight':['balanced']}

grid_rus=GridSearchCV(SVC(),param_grid,refit=True,verbose=2,cv=2)
grid_rus.fit(x_train_rus,y_train_rus)

print(grid_rus.best_estimator_)

grid_predicton_rus=grid_rus.predict(x_test_rus)

confusion_matrix(y_test_rus,grid_predicton_rus)

print(classification_report(y_test_rus,grid_predicton_rus))

"""# Model with Random Over Sampling"""

svc_ros=SVC()

svc_ros.fit(x_train_ros,y_train_ros)

y_pred_ros=svc_ros.predict(x_test_ros)

"""# Model Accuracy"""

confusion_matrix(y_test_ros,y_pred_ros)

print(classification_report(y_test_ros,y_pred_ros))

"""# Hyperparameter Tunning"""

param_grid={'C':[0.1,1,10],
            'gamma':[1,0.1,0.01],
            'kernel':['rbf'],
            'class_weight':['balanced']}

grid_ros = GridSearchCV(SVC(),param_grid,refit=True,verbose=2,cv=2)
grid_ros.fit(x_train_ros,y_train_ros)

print(grid_ros.best_estimator_)

grid_predicton_ros=grid_ros.predict(x_test_ros)

confusion_matrix(y_test_ros,grid_predicton_ros)

print(classification_report(y_test_ros,grid_predicton_ros))

"""# Lets Compare"""

print(classification_report(y_test,y_pred))

print(classification_report(y_test,grid_predicton))

print(classification_report(y_test_rus,y_pred_rus))

print(classification_report(y_test_rus,grid_predicton_rus))

print(classification_report(y_test_ros,y_pred_ros))

print(classification_report(y_test_ros,grid_predicton_ros))